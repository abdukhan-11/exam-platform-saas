{"_id":"ollama-ai-provider","_rev":"24-e9ecc186aa3f87d007dca8f0f0df61aa","name":"ollama-ai-provider","dist-tags":{"latest":"1.2.0"},"versions":{"0.1.0":{"name":"ollama-ai-provider","version":"0.1.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.1.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider#readme","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"4d2b9fc223e79d97472a557e0cd1c9e5489fb9e0","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.1.0.tgz","fileCount":9,"integrity":"sha512-XyuHTnPSDN1I3SByuV0X6fNAdn64u19oE/BXAt1nMWyd2nKG/7uGZ+uNuwYRXxiAFIyQIFb75ip2OMqTHGcVYw==","signatures":[{"sig":"MEYCIQDb2PS69/O7ASOgHc/KUPr5SbLSrrz4IUJxLb+XbyQ/hwIhANZU9PxJiYziid2YwBwm7NjxVlcREzlk4W5B7UTbRKfD","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":66703},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.1.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","changeset":"changeset","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","ci:release":"pnpm clean && pnpm build && changeset publish","ci:version":"changeset version && pnpm install --no-frozen-lockfile","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/c7145308e8aea188fd5f5ee0c6bff55d/ollama-ai-provider-0.1.0.tgz","_integrity":"sha512-XyuHTnPSDN1I3SByuV0X6fNAdn64u19oE/BXAt1nMWyd2nKG/7uGZ+uNuwYRXxiAFIyQIFb75ip2OMqTHGcVYw==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"@ai-sdk/provider":"0.0.3","@ai-sdk/provider-utils":"0.0.5"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","husky":"^9.0.11","eslint":"^8.57.0","vitest":"^1.6.0","prettier":"^3.2.5","typescript":"5.1.3","@types/node":"^18.19.31","lint-staged":"^15.2.2","@changesets/cli":"^2.27.1","@commitlint/cli":"^19.3.0","@edge-runtime/vm":"^3.2.0","eslint-plugin-sort":"^3.0.2","vite-tsconfig-paths":"^4.3.2","eslint-plugin-import":"^2.29.1","eslint-plugin-unicorn":"^52.0.0","eslint-config-prettier":"^9.1.0","eslint-plugin-prettier":"^5.1.3","@typescript-eslint/parser":"^7.8.0","eslint-plugin-unused-imports":"^3.2.0","@commitlint/config-conventional":"^19.2.2","@typescript-eslint/eslint-plugin":"^7.8.0","eslint-plugin-simple-import-sort":"^12.1.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.1.0_1714901681638_0.9888299937447771","host":"s3://npm-registry-packages"}},"0.2.0":{"name":"ollama-ai-provider","version":"0.2.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.2.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider#readme","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"42cdb654db82da9c68bd69a1f28e760666850a4a","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.2.0.tgz","fileCount":9,"integrity":"sha512-9NZ5QBjwgmSnDd4m0iBoPz3p7EenCwPV3tcYF0UOpkNjURaYabQQbW5F6V9+qTcN7dYo1HjobcaBNklqYttaXw==","signatures":[{"sig":"MEQCIDXhvDSPVgazWNWm6AVsPL7MYTKVaMdpu39mjNIdXni9AiARSdNj0SwiDFNkooc5tkSBFqLKOa4L8EbD0f7ddFsGlw==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":65881},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.2.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","changeset":"changeset","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","ci:release":"pnpm clean && pnpm build && changeset publish","ci:version":"changeset version && pnpm install --no-frozen-lockfile","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/aa1b2f068e9be3109fc1c9f6955f2d62/ollama-ai-provider-0.2.0.tgz","_integrity":"sha512-9NZ5QBjwgmSnDd4m0iBoPz3p7EenCwPV3tcYF0UOpkNjURaYabQQbW5F6V9+qTcN7dYo1HjobcaBNklqYttaXw==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"@ai-sdk/provider":"0.0.3","@ai-sdk/provider-utils":"0.0.5"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","husky":"^9.0.11","eslint":"^8.57.0","vitest":"^1.6.0","prettier":"^3.2.5","typescript":"5.1.3","@types/node":"^18.19.31","lint-staged":"^15.2.2","@changesets/cli":"^2.27.1","@commitlint/cli":"^19.3.0","@edge-runtime/vm":"^3.2.0","eslint-plugin-sort":"^3.0.2","vite-tsconfig-paths":"^4.3.2","eslint-plugin-import":"^2.29.1","eslint-plugin-unicorn":"^52.0.0","eslint-config-prettier":"^9.1.0","eslint-plugin-prettier":"^5.1.3","@typescript-eslint/parser":"^7.8.0","eslint-plugin-unused-imports":"^3.2.0","@commitlint/config-conventional":"^19.2.2","@typescript-eslint/eslint-plugin":"^7.8.0","eslint-plugin-simple-import-sort":"^12.1.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.2.0_1714970856993_0.7571094889646741","host":"s3://npm-registry-packages"}},"0.3.0":{"name":"ollama-ai-provider","version":"0.3.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.3.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider#readme","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"0a547448c36900a214d226e51a5af4e2472ef0d5","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.3.0.tgz","fileCount":9,"integrity":"sha512-4yPuvz75rTxDuvx5+u9Vr7lcjg+FQMJEg3t10IR6cVJCD0aA23zsoIlt5ecqfI63JB9/24rcRD/Ir7ZYmF4Riw==","signatures":[{"sig":"MEUCIQDEVvCeMrZi9ac4070cLlhkq0pM+m8fR2Orz9LQTrwiOgIgTvQa4ivbPvMNn+YrmbS0SpBDiaGDJdPchWLj3/OzrMw=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":70011},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.3.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","changeset":"changeset","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","ci:release":"pnpm clean && pnpm build && changeset publish","ci:version":"changeset version && pnpm install --no-frozen-lockfile","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/86731616ec99d95ff6cc74e357ba1222/ollama-ai-provider-0.3.0.tgz","_integrity":"sha512-4yPuvz75rTxDuvx5+u9Vr7lcjg+FQMJEg3t10IR6cVJCD0aA23zsoIlt5ecqfI63JB9/24rcRD/Ir7ZYmF4Riw==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"@ai-sdk/provider":"0.0.3","@ai-sdk/provider-utils":"0.0.5"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","husky":"^9.0.11","eslint":"^8.57.0","vitest":"^1.6.0","prettier":"^3.2.5","typescript":"5.1.3","@types/node":"^18.19.31","lint-staged":"^15.2.2","@changesets/cli":"^2.27.1","@commitlint/cli":"^19.3.0","@edge-runtime/vm":"^3.2.0","eslint-plugin-sort":"^3.0.2","vite-tsconfig-paths":"^4.3.2","eslint-plugin-import":"^2.29.1","eslint-plugin-unicorn":"^52.0.0","eslint-config-prettier":"^9.1.0","eslint-plugin-prettier":"^5.1.3","@typescript-eslint/parser":"^7.8.0","eslint-plugin-unused-imports":"^3.2.0","@commitlint/config-conventional":"^19.2.2","@typescript-eslint/eslint-plugin":"^7.8.0","eslint-plugin-simple-import-sort":"^12.1.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.3.0_1715104995751_0.39044717046381083","host":"s3://npm-registry-packages"}},"0.4.0":{"name":"ollama-ai-provider","version":"0.4.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.4.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider#readme","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"a69ac488c8295cdc5eecf655bfd9651db9cf764b","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.4.0.tgz","fileCount":9,"integrity":"sha512-jWt1LbX7/BMJHMLsJKtSKxQXKWWSbu4cR8AA1L7n8BiBgjZLzr3tjLrCfPQO1GnbcxzqODBJq3XvpBTRcA2Q1Q==","signatures":[{"sig":"MEQCIAXChM7yDL1PbtyMihnoHw6gh5ij7dgj5ERBj6CXxXl7AiBMUCUFa8AJNcgN/ee7hykzlU3O0jfuMBnbLc+Vw5iHlQ==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":77149},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.4.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","changeset":"changeset","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","ci:release":"pnpm clean && pnpm build && changeset publish","ci:version":"changeset version && pnpm install --no-frozen-lockfile","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/c3126c92d90ebc7c9d0cee99b30be3bc/ollama-ai-provider-0.4.0.tgz","_integrity":"sha512-jWt1LbX7/BMJHMLsJKtSKxQXKWWSbu4cR8AA1L7n8BiBgjZLzr3tjLrCfPQO1GnbcxzqODBJq3XvpBTRcA2Q1Q==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"@ai-sdk/provider":"0.0.3","@ai-sdk/provider-utils":"0.0.6"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","husky":"^9.0.11","eslint":"^8.57.0","vitest":"^1.6.0","prettier":"^3.2.5","typescript":"5.1.3","@types/node":"^18.19.31","lint-staged":"^15.2.2","@changesets/cli":"^2.27.1","@commitlint/cli":"^19.3.0","@edge-runtime/vm":"^3.2.0","eslint-plugin-sort":"^3.0.2","vite-tsconfig-paths":"^4.3.2","eslint-plugin-import":"^2.29.1","eslint-plugin-unicorn":"^52.0.0","eslint-config-prettier":"^9.1.0","eslint-plugin-prettier":"^5.1.3","@typescript-eslint/parser":"^7.8.0","eslint-plugin-unused-imports":"^3.2.0","@commitlint/config-conventional":"^19.2.2","@typescript-eslint/eslint-plugin":"^7.8.0","eslint-plugin-simple-import-sort":"^12.1.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.4.0_1715365459316_0.41884458434131333","host":"s3://npm-registry-packages"}},"0.5.0":{"name":"ollama-ai-provider","version":"0.5.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.5.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider#readme","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"17adbd8edb068db07daa787ef0929ced0c40bfe8","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.5.0.tgz","fileCount":9,"integrity":"sha512-D3fX7ZwG1MYv+RQ+7fkInAjuWU+Y0uurgOGRj6HXFcbnTOVb9g3NqhANHsB/JvA8QsB6qn4NWPpKGZeHgvF/XQ==","signatures":[{"sig":"MEQCIGbB6xqyxdzz21KXzZCSGseqssypD+ZWAGGSxLyUVIJVAiBQXJYEMZG4OUTDOv+TRZX4dRoSinoXDt+zaaPcxU+6MQ==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":99796},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.5.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","changeset":"changeset","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","ci:release":"pnpm clean && pnpm build && changeset publish","ci:version":"changeset version && pnpm install --no-frozen-lockfile","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/cb4f7cdf86a0b901a705629ded0a73f3/ollama-ai-provider-0.5.0.tgz","_integrity":"sha512-D3fX7ZwG1MYv+RQ+7fkInAjuWU+Y0uurgOGRj6HXFcbnTOVb9g3NqhANHsB/JvA8QsB6qn4NWPpKGZeHgvF/XQ==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"@ai-sdk/provider":"0.0.3","@ai-sdk/provider-utils":"0.0.6"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","husky":"^9.0.11","eslint":"^8.57.0","vitest":"^1.6.0","prettier":"^3.2.5","typescript":"5.1.3","@types/node":"^18.19.33","lint-staged":"^15.2.2","@changesets/cli":"^2.27.1","@commitlint/cli":"^19.3.0","@edge-runtime/vm":"^3.2.0","eslint-plugin-sort":"^3.0.2","vite-tsconfig-paths":"^4.3.2","eslint-plugin-import":"^2.29.1","eslint-plugin-unicorn":"^52.0.0","eslint-config-prettier":"^9.1.0","eslint-plugin-prettier":"^5.1.3","@typescript-eslint/parser":"^7.8.0","eslint-plugin-unused-imports":"^3.2.0","@commitlint/config-conventional":"^19.2.2","@typescript-eslint/eslint-plugin":"^7.8.0","eslint-plugin-simple-import-sort":"^12.1.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.5.0_1715448498233_0.5362560360089956","host":"s3://npm-registry-packages"}},"0.5.1":{"name":"ollama-ai-provider","version":"0.5.1","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.5.1","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider#readme","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"27af45d5ebc80f9759edf403c6ba64ac30cba9ba","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.5.1.tgz","fileCount":9,"integrity":"sha512-cMDRWIAfhvk2zyPc6Dwy2ZSdD1JfbTkoaO7o5oI6PBeI1hH/xsGGRYUJpvxxr73uATbulZB2itBC6zg/RqUjYQ==","signatures":[{"sig":"MEUCIDglrnr3ObUlEcb3+TQ0Ip4JtICC+1awU9Vs7uC+c9uyAiEA5vtdz0e+YWnwgBNfPT/wXGED8ZXTtTwPt/zUWXAJqN8=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":99974},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.5.1.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","changeset":"changeset","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","ci:release":"pnpm clean && pnpm build && changeset publish","ci:version":"changeset version && pnpm install --no-frozen-lockfile","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/32dbcded439fbf8989ed7e885733119c/ollama-ai-provider-0.5.1.tgz","_integrity":"sha512-cMDRWIAfhvk2zyPc6Dwy2ZSdD1JfbTkoaO7o5oI6PBeI1hH/xsGGRYUJpvxxr73uATbulZB2itBC6zg/RqUjYQ==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"@ai-sdk/provider":"0.0.5","@ai-sdk/provider-utils":"0.0.8"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","husky":"^9.0.11","eslint":"^8.57.0","vitest":"^1.6.0","prettier":"^3.2.5","typescript":"5.1.3","@types/node":"^18.19.33","lint-staged":"^15.2.2","@changesets/cli":"^2.27.1","@commitlint/cli":"^19.3.0","@edge-runtime/vm":"^3.2.0","eslint-plugin-sort":"^3.0.2","vite-tsconfig-paths":"^4.3.2","eslint-plugin-import":"^2.29.1","eslint-plugin-unicorn":"^52.0.0","eslint-config-prettier":"^9.1.0","eslint-plugin-prettier":"^5.1.3","@typescript-eslint/parser":"^7.9.0","eslint-plugin-unused-imports":"^3.2.0","@commitlint/config-conventional":"^19.2.2","@typescript-eslint/eslint-plugin":"^7.9.0","eslint-plugin-simple-import-sort":"^12.1.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.5.1_1715705299543_0.5146108488831334","host":"s3://npm-registry-packages"}},"0.6.0":{"name":"ollama-ai-provider","version":"0.6.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.6.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider#readme","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"ef003a90a9fe1ec334467faf2d69e559f3dfbba7","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.6.0.tgz","fileCount":9,"integrity":"sha512-LOV+1sjtIVNNHnTAPis369SBmmdAHa8Mz3JfDJ81YMjRiqfSVvQYCoMQqCiF52/WVRtJ6hSjzjbVjP6T1fvHSg==","signatures":[{"sig":"MEUCIDU/pKebkXoWAGltDrAmoVCaa+SeBPV5TTuqUcO2y9zoAiEA5iqt77m04TtBERjLDtQupAA2K0E2FgVo6MKI9t9dm/I=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":117072},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.6.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","changeset":"changeset","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","ci:release":"pnpm clean && pnpm build && changeset publish","ci:version":"changeset version && pnpm install --no-frozen-lockfile","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/9be65c1a45648956de3269532b6cc2bc/ollama-ai-provider-0.6.0.tgz","_integrity":"sha512-LOV+1sjtIVNNHnTAPis369SBmmdAHa8Mz3JfDJ81YMjRiqfSVvQYCoMQqCiF52/WVRtJ6hSjzjbVjP6T1fvHSg==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"@ai-sdk/provider":"0.0.5","@ai-sdk/provider-utils":"0.0.8"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","husky":"^9.0.11","eslint":"^8.57.0","vitest":"^1.6.0","prettier":"^3.2.5","typescript":"5.1.3","@types/node":"^18.19.33","lint-staged":"^15.2.2","@changesets/cli":"^2.27.1","@commitlint/cli":"^19.3.0","@edge-runtime/vm":"^3.2.0","eslint-plugin-sort":"^3.0.2","vite-tsconfig-paths":"^4.3.2","eslint-plugin-import":"^2.29.1","eslint-plugin-unicorn":"^52.0.0","eslint-config-prettier":"^9.1.0","eslint-plugin-prettier":"^5.1.3","@typescript-eslint/parser":"^7.9.0","eslint-plugin-unused-imports":"^3.2.0","@commitlint/config-conventional":"^19.2.2","@typescript-eslint/eslint-plugin":"^7.9.0","eslint-plugin-simple-import-sort":"^12.1.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.6.0_1715804438942_0.13850161245275316","host":"s3://npm-registry-packages"}},"0.7.0":{"name":"ollama-ai-provider","version":"0.7.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.7.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"dist":{"shasum":"2151391b77d7ae26846e0ac503a4477effc4742c","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.7.0.tgz","fileCount":8,"integrity":"sha512-mw2NusrPkBJ7OxBLdRNNIVbXKtil//mj/7n393+9kj74gT6EKzyjf+j2v9AvFi0fJwnuW1YxnJpbuw1UjfdPRQ==","signatures":[{"sig":"MEYCIQDcdOowbrOqgj7H+WQj9TJXrRSJbvYra+HkLEJaaFlPhwIhALWgIsQEDb3gMZoJ4sR0m8kDV11Y5aQ5VQSJqB0EjF1N","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":132632},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.7.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/ac09b023675107d6e1fc3cad1e089216/ollama-ai-provider-0.7.0.tgz","_integrity":"sha512-mw2NusrPkBJ7OxBLdRNNIVbXKtil//mj/7n393+9kj74gT6EKzyjf+j2v9AvFi0fJwnuW1YxnJpbuw1UjfdPRQ==","_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"^0.1.7","@ai-sdk/provider":"0.0.5","@ai-sdk/provider-utils":"0.0.8"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.0.2","typescript":"5.1.3","@types/node":"^18.19.33","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.7.0_1716106811581_0.4441471682266309","host":"s3://npm-registry-packages"}},"0.8.0":{"name":"ollama-ai-provider","version":"0.8.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.8.0","dist":{"shasum":"7e23c8f801d6afacc531eb4e0ee4a7b95921f01b","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.8.0.tgz","fileCount":8,"integrity":"sha512-HWAfsYWYcvp7/tGanOSHGIJ0qWEsJgVs2To0Ki7O3T8JiyakcwyRZ7qjZkvfpp03In9XyT4LHNOlLFSECoq2xQ==","signatures":[{"sig":"MEUCIHTCgMXUUYSG73iBwTI8/gzbytoyrYo/jE/uWQoLo+CbAiEA7RjdUAJWKKnKIO82EpOgf7GKPVxM0EFbFax+9qfEhKE=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":128897},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.8.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/99e4601bb386a8b1e1aaf739077b9ea0/ollama-ai-provider-0.8.0.tgz","_integrity":"sha512-HWAfsYWYcvp7/tGanOSHGIJ0qWEsJgVs2To0Ki7O3T8JiyakcwyRZ7qjZkvfpp03In9XyT4LHNOlLFSECoq2xQ==","_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"^0.1.7","@ai-sdk/provider":"0.0.11","@ai-sdk/provider-utils":"1.0.0"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.1.0","typescript":"5.1.3","@types/node":"^18.19.39","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.8.0_1719599126240_0.8209749979203833","host":"s3://npm-registry-packages"}},"0.9.0":{"name":"ollama-ai-provider","version":"0.9.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.9.0","homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"de5724f5acd778281cb29e42b18225e637acef8f","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.9.0.tgz","fileCount":9,"integrity":"sha512-RpxjGLtyjj0jmsw5XL7zxbnGA6dRh9iWouV2cAcS1PFTVYRAyavBHeWxefZx4U+75m2tdv6U78wGTuy6PzyEsw==","signatures":[{"sig":"MEYCIQDKCDOVbMy/6Q3GU97cHVWqhFXMcKG9eXaKbFU10CpGrQIhAMGojwahG/I34Ct4iN5I6Av6GE1EQDf1OwMk7+EbO9vG","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":133767},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.9.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/908878a55a8f26229e83324d6ab2c707/ollama-ai-provider-0.9.0.tgz","_integrity":"sha512-RpxjGLtyjj0jmsw5XL7zxbnGA6dRh9iWouV2cAcS1PFTVYRAyavBHeWxefZx4U+75m2tdv6U78wGTuy6PzyEsw==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"^0.1.7","@ai-sdk/provider":"0.0.11","@ai-sdk/provider-utils":"1.0.0"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.1.0","typescript":"5.1.3","@types/node":"^18.19.39","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.9.0_1719652732133_0.10902694488350062","host":"s3://npm-registry-packages"}},"0.9.1":{"name":"ollama-ai-provider","version":"0.9.1","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.9.1","homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"d85c1cea160f78df19a6ab707992eda87b0adf4e","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.9.1.tgz","fileCount":9,"integrity":"sha512-aIiuRiXAI0XFWIyqL5Zo2Mok2aFzEpr7NbtNucGew6Jhmsfzg9NPhVWQy+5MhnaTtgp+X+2zlOqjI5Yfn5/dDg==","signatures":[{"sig":"MEUCIQCMMl58vQLHm9vaKttii+Fgf7FwEvOE/IORWljAAyfzPAIgImxEgBP+VniL5smdi47nYemXUAnxDNvbqRrwaezFU+M=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":133541},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.9.1.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/95f4cae5ada49067465b3dd10b512809/ollama-ai-provider-0.9.1.tgz","_integrity":"sha512-aIiuRiXAI0XFWIyqL5Zo2Mok2aFzEpr7NbtNucGew6Jhmsfzg9NPhVWQy+5MhnaTtgp+X+2zlOqjI5Yfn5/dDg==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"^0.1.7","@ai-sdk/provider":"0.0.11","@ai-sdk/provider-utils":"1.0.0"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.1.0","typescript":"5.1.3","@types/node":"^18.19.39","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.9.1_1719736639478_0.8926099033847885","host":"s3://npm-registry-packages"}},"0.10.0":{"name":"ollama-ai-provider","version":"0.10.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.10.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"e48586efd3a86fc9c88fe2bf0e167fb541c452a1","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.10.0.tgz","fileCount":9,"integrity":"sha512-ppzofC1JtB8yQ2TRAGm6I9ppoRSpXZGQV4u5ez2JAQ5hwnkJKULgEg5Hc4rt84ErF3MpEIpG1ZSBV4njlL6LPQ==","signatures":[{"sig":"MEUCIQDlXIrY7G8M5ZwH5uIU+/eLCHsR2tStcHW5y2F72TSnywIgK84hLHmgKorH9wjDCJKHWI0WqtZXIHtVmKTsYl5U6rc=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":142068},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.10.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/71d43f6b360ed5f6bd5f6749830271ab/ollama-ai-provider-0.10.0.tgz","_integrity":"sha512-ppzofC1JtB8yQ2TRAGm6I9ppoRSpXZGQV4u5ez2JAQ5hwnkJKULgEg5Hc4rt84ErF3MpEIpG1ZSBV4njlL6LPQ==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"^0.1.7","@ai-sdk/provider":"0.0.11","@ai-sdk/provider-utils":"1.0.0"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.1.0","typescript":"5.1.3","@types/node":"^18.19.39","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.10.0_1720339919821_0.8747283138921109","host":"s3://npm-registry-packages"}},"0.11.0":{"name":"ollama-ai-provider","version":"0.11.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.11.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"15f4a94422eaeb35497e3bd7b5ee4dae602eb28c","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.11.0.tgz","fileCount":9,"integrity":"sha512-zobsS8pZJ7IUfqQF+QiqWBG92ocm4fHpV+iXUCXoFtNMSN65Nr3v0TyW2sTooku3BVm9EmOBeMenKSOIv7h5Sw==","signatures":[{"sig":"MEUCIQDQG8KJmOg58VOZiRSn/2P3praWBGp2sEsMAXqos4pQ4wIgUYpGhSzwlHHRSgJbVWFsehAcmFWs9ErnH2gVVJ7WJTY=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":134755},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.11.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/private/var/folders/h9/pn9w4vv17cz9v6ksptg9s1gr0000gn/T/6c4a7da4e8a649772e43ebf6bad0a2e3/ollama-ai-provider-0.11.0.tgz","_integrity":"sha512-zobsS8pZJ7IUfqQF+QiqWBG92ocm4fHpV+iXUCXoFtNMSN65Nr3v0TyW2sTooku3BVm9EmOBeMenKSOIv7h5Sw==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"18.20.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.14","@ai-sdk/provider-utils":"1.0.5"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.22.4","tsup":"^8.1.0","typescript":"5.1.3","@types/node":"^18.19.39","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.11.0_1722708420524_0.9212215144501372","host":"s3://npm-registry-packages"}},"0.12.0":{"name":"ollama-ai-provider","version":"0.12.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.12.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"7fc7aa5705091298cc4b8acbbada824185598e40","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.12.0.tgz","fileCount":9,"integrity":"sha512-m9/CSY4ggCM7qMJ2oUYhOxmG2zu4hzp5Dot3qLLn0Q6Tq/0AqF8ZUs82A+YWnu358GfvMqVthimMXt0poeNbJg==","signatures":[{"sig":"MEUCIQDRZ2cKUFZ6SXc3JuwPeX35/QhrR1rJR0T2Xxpii5tJogIgQ+QY7J8wiAU9+HxjsBXFyi9RfJ5tG4C3n0dSqG+VHmg=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":140901},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.12.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/private/var/folders/h9/pn9w4vv17cz9v6ksptg9s1gr0000gn/T/86fa617e4908a7041795c83f6469068f/ollama-ai-provider-0.12.0.tgz","_integrity":"sha512-m9/CSY4ggCM7qMJ2oUYhOxmG2zu4hzp5Dot3qLLn0Q6Tq/0AqF8ZUs82A+YWnu358GfvMqVthimMXt0poeNbJg==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"18.20.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.15","@ai-sdk/provider-utils":"1.0.7"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.2.4","typescript":"5.1.3","@types/node":"^18.19.43","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.12.0_1723021146106_0.5205834969898027","host":"s3://npm-registry-packages"}},"0.12.1":{"name":"ollama-ai-provider","version":"0.12.1","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.12.1","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"bd22b9abe5703a2f27a61f9a427c0259e85f13f8","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.12.1.tgz","fileCount":9,"integrity":"sha512-WqCLEgqTwgEaO5kOlndHfCJawDLP2tbIicLy6rSKR+KrVDiLkYiOdTfk1eVMvm5KTDUYGXZBcowhQC17Ceayag==","signatures":[{"sig":"MEQCIF1EcGo3/mHICUu2haPvxllGmlE5wZgbLsHVo7xLAYRQAiBE9VLpPpUw7ZjD2F/oUa5AcJho2M4O6Vc+dyl/9PgXug==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":141220},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.12.1.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/private/var/folders/h9/pn9w4vv17cz9v6ksptg9s1gr0000gn/T/ec788f094a0a291c5457f2025167d15e/ollama-ai-provider-0.12.1.tgz","_integrity":"sha512-WqCLEgqTwgEaO5kOlndHfCJawDLP2tbIicLy6rSKR+KrVDiLkYiOdTfk1eVMvm5KTDUYGXZBcowhQC17Ceayag==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"18.20.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.21","@ai-sdk/provider-utils":"1.0.16"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.2.4","typescript":"5.5.4","@types/node":"^18.19.43","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.12.1_1724351468323_0.09907183697296507","host":"s3://npm-registry-packages"}},"0.13.0":{"name":"ollama-ai-provider","version":"0.13.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.13.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"348bd6e09127f734bd1de9a59da0660cbaa9688e","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.13.0.tgz","fileCount":9,"integrity":"sha512-ZEtKD6ixxLIVLVRIZMh4yl0HGcdc8RotJW4ncdeAYXYT3fJ3LMvbatv9x8BeFkeb6+xy8Smc+4DbPMVFT+cjug==","signatures":[{"sig":"MEQCIDUtGuoHjiPKdq29770/dstebCoBZ3UBU2b5IUoCN+xsAiBsRurwOonKUnbelbYK/FxLy+0+HomHDzen7N9NOKYHjg==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":141356},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.13.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/70f9dd33fb6c947ee038468750ce43f8/ollama-ai-provider-0.13.0.tgz","_integrity":"sha512-ZEtKD6ixxLIVLVRIZMh4yl0HGcdc8RotJW4ncdeAYXYT3fJ3LMvbatv9x8BeFkeb6+xy8Smc+4DbPMVFT+cjug==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.22","@ai-sdk/provider-utils":"1.0.17"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.2.4","typescript":"5.5.4","@types/node":"^18.19.46","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.13.0_1724775062858_0.8076062601255289","host":"s3://npm-registry-packages"}},"0.14.0":{"name":"ollama-ai-provider","version":"0.14.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.14.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"3d1b543772beab141ea8f7d2387a170a95e7fc58","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.14.0.tgz","fileCount":9,"integrity":"sha512-ARN1pdN8v1lj9ekKml1rjGrdlh2fhUNrhRcUwf1APM/Wh+LguZ4wqF1Vf+x/Pqg27ChaMpd9GNnVuvwtqGxQ2g==","signatures":[{"sig":"MEQCIAUJfmOQXwNXl0VFM2GXktuF+li6QFT4UK6iu2ZzA3XGAiAeuMp+d98n9xne6hqS52wfXdNUCvrnb+4Ym34CvmeC+A==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":141356},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.14.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/06dc0845be52fd41884477a779b9d44b/ollama-ai-provider-0.14.0.tgz","_integrity":"sha512-ARN1pdN8v1lj9ekKml1rjGrdlh2fhUNrhRcUwf1APM/Wh+LguZ4wqF1Vf+x/Pqg27ChaMpd9GNnVuvwtqGxQ2g==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.23","@ai-sdk/provider-utils":"1.0.18"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.2.4","typescript":"5.5.4","@types/node":"^18.19.46","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.14.0_1725697287472_0.10462858631394045","host":"s3://npm-registry-packages"}},"0.15.0":{"name":"ollama-ai-provider","version":"0.15.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.15.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"b009e8657693de7f342e6f3937e9e260805bb52b","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.15.0.tgz","fileCount":9,"integrity":"sha512-pBRv2PjOPFdjB2fxOcu4dV3oT6NUxUI+V3c0Cu3r8Dwv6WmhHkRGLMscyRMU4Q2YVAtbghrvkfBGQmaqpMh/KQ==","signatures":[{"sig":"MEUCIQC3FBspC1bDP4dTW3/elNqVVTjZRAJn8fPMhuLkc9RrrAIgLPQeQWRKS5ojWEn5+lZVlUfCnBdhbc1mxRk9D/xGiTY=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":148582},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.15.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/2bad5753dc964b74a00afdb4cbbcb494/ollama-ai-provider-0.15.0.tgz","_integrity":"sha512-pBRv2PjOPFdjB2fxOcu4dV3oT6NUxUI+V3c0Cu3r8Dwv6WmhHkRGLMscyRMU4Q2YVAtbghrvkfBGQmaqpMh/KQ==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.23","@ai-sdk/provider-utils":"1.0.18"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.2.4","typescript":"5.5.4","@types/node":"^18.19.46","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.15.0_1726261309905_0.30819160832847814","host":"s3://npm-registry-packages"}},"0.15.1":{"name":"ollama-ai-provider","version":"0.15.1","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.15.1","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"45f4598eeee5441886bc2442eaad596d637e1033","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.15.1.tgz","fileCount":9,"integrity":"sha512-eP1vxeJUgf1TdVyvJFqHEgU7eSnG7QYam4w1zTl4tokDh5eRw9aQtcdHYpq9xd5yU9b805v/Ghba6nVy1zOUew==","signatures":[{"sig":"MEUCIQCaCBMsFMJAttwWXD5kLkJJtb7JY8t1Wo6QEiKlaNsvrAIgBjZeV5VrLscDQ4RU3pxTCNyTutyWt5TSdxkVirVefeU=","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":149112},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.15.1.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/private/var/folders/h9/pn9w4vv17cz9v6ksptg9s1gr0000gn/T/184a7454a51768357120bd5f173ef13a/ollama-ai-provider-0.15.1.tgz","_integrity":"sha512-eP1vxeJUgf1TdVyvJFqHEgU7eSnG7QYam4w1zTl4tokDh5eRw9aQtcdHYpq9xd5yU9b805v/Ghba6nVy1zOUew==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"18.20.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.23","@ai-sdk/provider-utils":"1.0.19"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.2.4","typescript":"5.5.4","@types/node":"^18.19.46","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.15.1_1727105597971_0.3241883582208711","host":"s3://npm-registry-packages"}},"0.15.2":{"name":"ollama-ai-provider","version":"0.15.2","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.15.2","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"fa2cde1f0aa5e93aa4e201466a5fdbc9a65e1382","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.15.2.tgz","fileCount":9,"integrity":"sha512-bMDUlYmohulD87Xrv6meuftQdmFTygtrQywy6/gqdf1bTsJFP1VCx3MrisLFBzb4mMOj02NER7yZhiGIlAx30w==","signatures":[{"sig":"MEQCICjp7gwQCiyoIaCyUQPhY03cNb4dTucKIUFmiXIImIUhAiAGt/u4GI/AVRiInzTDuB3anUFB+Q1mEt+oD2M8nmZqOA==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":149202},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.15.2.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/1d0929fc65be12f7f0e687ee58a05391/ollama-ai-provider-0.15.2.tgz","_integrity":"sha512-bMDUlYmohulD87Xrv6meuftQdmFTygtrQywy6/gqdf1bTsJFP1VCx3MrisLFBzb4mMOj02NER7yZhiGIlAx30w==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.24","@ai-sdk/provider-utils":"1.0.20"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.2.4","typescript":"5.5.4","@types/node":"^18.19.46","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.15.2_1728581491697_0.8204153283565987","host":"s3://npm-registry-packages"}},"0.16.0":{"name":"ollama-ai-provider","version":"0.16.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.16.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"d80fbaff7f6aec016ae3a910d9c47c44a9a386ca","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.16.0.tgz","fileCount":9,"integrity":"sha512-8M04rSNl2tsZXzPvttUAkn5nptLlZXvGW+I/MYY6dECSE2C1ERONxgX15urYiJKpDNMniI+H/aILpFD0RFe/4g==","signatures":[{"sig":"MEQCIAYMWSCWDP4Me5WWBn3f5XqN37ePqoJ7yEDHKWrD2FrzAiA46oxVrE3UXrC7JS4Rkz5eLMfB/J0A5Z3tMKnoRlw86A==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":157134},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.16.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/87b39ff2f45ea6db61b5f2122ee06b76/ollama-ai-provider-0.16.0.tgz","_integrity":"sha512-8M04rSNl2tsZXzPvttUAkn5nptLlZXvGW+I/MYY6dECSE2C1ERONxgX15urYiJKpDNMniI+H/aILpFD0RFe/4g==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.26","@ai-sdk/provider-utils":"1.0.22"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.3.0","typescript":"5.5.4","@types/node":"^18.19.56","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.16.0_1730025112432_0.9662375774176974","host":"s3://npm-registry-packages"}},"0.16.1":{"name":"ollama-ai-provider","version":"0.16.1","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@0.16.1","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"3238d4d36c630ed7f152e3e1cf24b68eb0c244d4","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-0.16.1.tgz","fileCount":9,"integrity":"sha512-0vSQVz5Y/LguyzfO4bi1JrrVGF/k2JvO8/uFR0wYmqDFp8KPp4+AhdENSynGBr1oRhMWOM4F1l6cv7UNDgRMjw==","signatures":[{"sig":"MEYCIQCVICqPIJUPi6wEeg4xkvteXDSdZKv4O4/zqv7e6dqASwIhAJ1OOMV5cjsftTNpyLdGwQzoIKHlYZJ92+f44IE/ZmIX","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":157846},"main":"./dist/index.js","_from":"file:ollama-ai-provider-0.16.1.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/92a1a869f85782acd742f5615ccb4886/ollama-ai-provider-0.16.1.tgz","_integrity":"sha512-0vSQVz5Y/LguyzfO4bi1JrrVGF/k2JvO8/uFR0wYmqDFp8KPp4+AhdENSynGBr1oRhMWOM4F1l6cv7UNDgRMjw==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"0.0.26","@ai-sdk/provider-utils":"1.0.22"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.3.0","typescript":"5.5.4","@types/node":"^18.19.56","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_0.16.1_1730933599734_0.3077772227490858","host":"s3://npm-registry-packages"}},"1.0.0":{"name":"ollama-ai-provider","version":"1.0.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@1.0.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"17115de6d752e58a4efc350f69e41636b2d56151","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-1.0.0.tgz","fileCount":9,"integrity":"sha512-72FhNSjS/yav+w1jGX7K/VX0Psv9C0TA1aXk6K2sEloz542AJCYqHIUu8Jt0TWhoB98L9pBZwV8Vr89eBavyzg==","signatures":[{"sig":"MEQCIBX/82ojpPdtfvObZd1gFgDizQCxDGTLQZ/08yygKX1VAiBE+HhlHx5q6+tGrdPCyVF9naMUB25DSfmgJ2oElBE/tA==","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":149678},"main":"./dist/index.js","_from":"file:ollama-ai-provider-1.0.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/tmp/920eafa796760a282f0145e00b18eb1f/ollama-ai-provider-1.0.0.tgz","_integrity":"sha512-72FhNSjS/yav+w1jGX7K/VX0Psv9C0TA1aXk6K2sEloz542AJCYqHIUu8Jt0TWhoB98L9pBZwV8Vr89eBavyzg==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"20.12.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"^1.0.0","@ai-sdk/provider-utils":"^2.0.0"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.3.0","typescript":"5.6.3","@types/node":"^18.19.56","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_1.0.0_1732301397924_0.08326237556043825","host":"s3://npm-registry-packages"}},"1.1.0":{"name":"ollama-ai-provider","version":"1.1.0","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","_id":"ollama-ai-provider@1.1.0","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"homepage":"https://github.com/sgomez/ollama-ai-provider","bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"dist":{"shasum":"30faa14c68fef55b515adeccc22d4af012f8b6aa","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-1.1.0.tgz","fileCount":9,"integrity":"sha512-kpNLX2I0qVZfyML/d+o3UZrJxUokWeLrrQeuAQOuYPKmZcN9bpU7PD7SuxsH86krWvXqM0SJygTPa0OwEK71OQ==","signatures":[{"sig":"MEYCIQCa1SwsFRaDFbgYEfF9Fi9+ikbkCQXT/WrYO3DLQLTJwwIhAJOQBOvG9T5FOB7clgAmPSVDfmcuiGKLdnFZSd85Lews","keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA"}],"unpackedSize":152015},"main":"./dist/index.js","_from":"file:ollama-ai-provider-1.1.0.tgz","types":"./dist/index.d.ts","module":"./dist/index.mjs","engines":{"node":">=18"},"scripts":{"dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","build":"tsup","clean":"rm -rf dist","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\""},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"_resolved":"/private/var/folders/h9/pn9w4vv17cz9v6ksptg9s1gr0000gn/T/eb1345cdc48d66c1beb8e3ebad11bf24/ollama-ai-provider-1.1.0.tgz","_integrity":"sha512-kpNLX2I0qVZfyML/d+o3UZrJxUokWeLrrQeuAQOuYPKmZcN9bpU7PD7SuxsH86krWvXqM0SJygTPa0OwEK71OQ==","repository":{"url":"git+https://github.com/sgomez/ollama-ai-provider.git","type":"git"},"_npmVersion":"10.5.0","description":"Vercel AI Provider for running LLMs locally using Ollama","directories":{},"_nodeVersion":"18.20.2","dependencies":{"partial-json":"0.1.7","@ai-sdk/provider":"^1.0.0","@ai-sdk/provider-utils":"^2.0.0"},"publishConfig":{"access":"public"},"_hasShrinkwrap":false,"devDependencies":{"zod":"3.23.8","tsup":"^8.3.0","typescript":"5.6.3","@types/node":"^18.19.56","@edge-runtime/vm":"^3.2.0"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"_npmOperationalInternal":{"tmp":"tmp/ollama-ai-provider_1.1.0_1733526596431_0.1326144003673304","host":"s3://npm-registry-packages"}},"1.2.0":{"name":"ollama-ai-provider","version":"1.2.0","description":"Vercel AI Provider for running LLMs locally using Ollama","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","keywords":["ai","vercel-ai"],"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","dependencies":{"@ai-sdk/provider":"^1.0.0","@ai-sdk/provider-utils":"^2.0.0","partial-json":"0.1.7"},"devDependencies":{"@edge-runtime/vm":"^3.2.0","@types/node":"^18.19.56","tsup":"^8.3.0","typescript":"5.6.3","zod":"3.23.8"},"peerDependencies":{"zod":"^3.0.0"},"peerDependenciesMeta":{"zod":{"optional":true}},"engines":{"node":">=18"},"publishConfig":{"access":"public"},"homepage":"https://github.com/sgomez/ollama-ai-provider","repository":{"type":"git","url":"git+https://github.com/sgomez/ollama-ai-provider.git"},"bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"scripts":{"build":"tsup","clean":"rm -rf dist","dev":"tsup --watch","lint":"eslint \"./**/*.ts*\"","type-check":"tsc --noEmit","prettier-check":"prettier --check \"./**/*.ts*\"","test":"pnpm test:node && pnpm test:edge","test:edge":"vitest --config vitest.edge.config.js --run","test:node":"vitest --config vitest.node.config.js --run"},"_id":"ollama-ai-provider@1.2.0","_integrity":"sha512-jTNFruwe3O/ruJeppI/quoOUxG7NA6blG3ZyQj3lei4+NnJo7bi3eIRWqlVpRlu/mbzbFXeJSBuYQWF6pzGKww==","_resolved":"/tmp/0a8d030ab8da2b69e1c2dddf5e60c102/ollama-ai-provider-1.2.0.tgz","_from":"file:ollama-ai-provider-1.2.0.tgz","_nodeVersion":"20.12.2","_npmVersion":"10.5.0","dist":{"integrity":"sha512-jTNFruwe3O/ruJeppI/quoOUxG7NA6blG3ZyQj3lei4+NnJo7bi3eIRWqlVpRlu/mbzbFXeJSBuYQWF6pzGKww==","shasum":"b052c8ad96ef8048185a7ac01ee9351a0cedf0ce","tarball":"https://registry.npmjs.org/ollama-ai-provider/-/ollama-ai-provider-1.2.0.tgz","fileCount":9,"unpackedSize":161033,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEYCIQCpJA/Giztxw2+QjXWFp5zT9zi2RPRMhmSGml1Jp4qXMAIhAIULteBXq5FrmSwuFIiBe3VbxLlyhWM2leBH2n+sKqIc"}]},"_npmUser":{"name":"sgomez","email":"decano@gmail.com"},"directories":{},"maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages-npm-production","tmp":"tmp/ollama-ai-provider_1.2.0_1737132876556_0.1429458296335202"},"_hasShrinkwrap":false}},"time":{"created":"2024-05-05T09:34:41.490Z","modified":"2025-01-17T16:54:36.937Z","0.1.0":"2024-05-05T09:34:41.828Z","0.2.0":"2024-05-06T04:47:37.296Z","0.3.0":"2024-05-07T18:03:15.972Z","0.4.0":"2024-05-10T18:24:19.506Z","0.5.0":"2024-05-11T17:28:18.388Z","0.5.1":"2024-05-14T16:48:19.709Z","0.6.0":"2024-05-15T20:20:39.127Z","0.7.0":"2024-05-19T08:20:11.748Z","0.8.0":"2024-06-28T18:25:26.400Z","0.9.0":"2024-06-29T09:18:52.298Z","0.9.1":"2024-06-30T08:37:19.689Z","0.10.0":"2024-07-07T08:12:00.019Z","0.11.0":"2024-08-03T18:07:00.722Z","0.12.0":"2024-08-07T08:59:06.268Z","0.12.1":"2024-08-22T18:31:08.532Z","0.13.0":"2024-08-27T16:11:03.016Z","0.14.0":"2024-09-07T08:21:27.651Z","0.15.0":"2024-09-13T21:01:50.115Z","0.15.1":"2024-09-23T15:33:18.289Z","0.15.2":"2024-10-10T17:31:31.885Z","0.16.0":"2024-10-27T10:31:52.625Z","0.16.1":"2024-11-06T22:53:19.965Z","1.0.0":"2024-11-22T18:49:58.114Z","1.1.0":"2024-12-06T23:09:56.599Z","1.2.0":"2025-01-17T16:54:36.767Z"},"bugs":{"url":"https://github.com/sgomez/ollama-ai-provider/issues"},"author":{"name":"Sergio Gómez Bachiller","email":"decano@gmail.com"},"license":"Apache-2.0","homepage":"https://github.com/sgomez/ollama-ai-provider","keywords":["ai","vercel-ai"],"repository":{"type":"git","url":"git+https://github.com/sgomez/ollama-ai-provider.git"},"description":"Vercel AI Provider for running LLMs locally using Ollama","maintainers":[{"name":"sgomez","email":"decano@gmail.com"}],"readme":"# Ollama Provider for the Vercel AI SDK\n\nThe **[Ollama Provider](https://github.com/sgomez/ollama-ai-provider)** for the [Vercel AI SDK](https://sdk.vercel.ai/docs)\ncontains language model support for the Ollama APIs and embedding model support for the Ollama embeddings API.\n\n## Requirements\n\nThis provider requires Ollama >= 0.5.0\n\n## Setup\n\nThe Ollama provider is available in the `ollama-ai-provider` module. You can install it with\n\n```bash\nnpm i ollama-ai-provider\n```\n\n## Provider Instance\n\nYou can import the default provider instance `ollama` from `ollama-ai-provider`:\n\n```ts\nimport { ollama } from 'ollama-ai-provider';\n```\n\n## Example\n\n```ts\nimport { ollama } from 'ollama-ai-provider';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: ollama('phi3'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n## Documentation\n\nPlease check out the **[Ollama provider documentation](https://github.com/sgomez/ollama-ai-provider)** for more information.\n","readmeFilename":"README.md"}